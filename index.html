<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Dayoung  Gong</title>
    <meta name="author" content="Dayoung  Gong" />
    <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
" />
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/smile-o"/>
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://gongda0e.github.io/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark" />

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="https://gongda0e.github.io/"><span class="font-weight-bold">Dayoung</span>   Gong</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item active">
                <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a>
              </li>
              <!---->
              <!-- Blog -->
              <!--<li class="nav-item ">
                <a class="nav-link" href="/blog/">blog</a>
              </li>-->

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">Publications</a>
              </li>

              <!-- Toogle theme mode -->
              <div class="toggle-container">
                <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </a>
              </div>
            </ul>
          </div>
        </div>
      </nav>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      <!-- about.html -->
      <div class="post">
        <header class="post-header">
          <h1 class="post-title">
           <span class="font-weight-bold">Dayoung</span>  Gong
          </h1>
          <p class="desc">Integrated M.S. &amp; Ph.D. student in the <a href="http://cvlab.postech.ac.kr/lab/" target="_blank" rel="noopener noreferrer">Computer Vision Lab</a> at <a href="https://www.postech.ac.kr" target="_blank" rel="noopener noreferrer">POSTECH</a>.</p>
        </header>

        <article>
          <div class="profile float-right">
<figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/dygong-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/dygong-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/dygong-1400.webp"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid z-dept-1 rounded" src="/assets/img/dygong.png" alt="dygong.png">

  </picture>

</figure>

          </div>

          <div class="clearfix">
            <p>I am a graduate student in the Graduate School of Artificial Intelligence integrated M.S. &amp; Ph.D. program at <a href="https://www.postech.ac.kr" target="_blank" rel="noopener noreferrer">POSTECH</a>. I am a member of the <a href="http://cvlab.postech.ac.kr/lab/" target="_blank" rel="noopener noreferrer">Computer Vision Lab</a> at POSTECH, working with <a href="http://cvlab.postech.ac.kr/~mcho/" target="_blank" rel="noopener noreferrer">Minsu Cho</a>.
Currently, I am a visiting researcher at the <a href="http://imagine.enpc.fr" target="_blank" rel="noopener noreferrer">IMAGINE</a> group at <a href="https://ecoledesponts.fr" target="_blank" rel="noopener noreferrer">√âcole des Ponts ParisTech</a> (aka ENPC), working with <a href="https://gulvarol.github.io" target="_blank" rel="noopener noreferrer">G√ºl Varol</a>.
Previously, I completed my B.S. in Convergence IT Engineering at POSTECH.<br>
<br>
My research interests lie in computer vision and deep learning, especially in understanding temporal and semantic relations between actions in long-term videos.
I‚Äôve worked on long-term action anticipation.
If you are interested in my research projects, please feel free to contact me by clicking one of the icons below.</p>

<!-- Write your biography here. Tell the world about yourself. Link to your favorite [subreddit](http://reddit.com). You can put a picture in, too. The code is already in, just name your picture `prof_pic.jpg` and put it in the `img/` folder.

Put your address / P.O. box / other info right below your picture. You can also disable any these elements by editing `profile` property of the YAML header of your `_pages/about.md`. Edit `_bibliography/papers.bib` and Jekyll will render your [publications page](/al-folio/publications/) automatically.

Link to your social media connections, too. This theme is set up to use [Font Awesome icons](http://fortawesome.github.io/Font-Awesome/) and [Academicons](https://jpswalsh.github.io/academicons/), like the ones below. Add your Facebook, Twitter, LinkedIn, Google Scholar, or just disable all of them. -->

          <!-- Social -->
          <div class="social">
            <div class="contact-icons">
            <a href="mailto:%67%6F%6E%67%64%61%30%65@%70%6F%73%74%65%63%68.%61%63.%6B%72" title="email"><i class="fas fa-envelope"></i></a>
            <a href="https://scholar.google.com/citations?user=LU_q94AAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a>
            <a href="https://github.com/gongda0e" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a>
            <a href="https://www.linkedin.com/in/dayoung-gong-11120717a" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a>
            
            </div>
                
            <div class="contact-note">
              
            </div>
            
          </div>
          </div>

          <!-- News -->          
          <div class="news">
            <h2>News</h2>
            <div class="table-responsive">
              <table class="table table-sm table-borderless"> 
                <tr>
                  <th scope="row">Jun 25, 2025</th>
                  <td>
                    üìù Our paper ‚ÄòGeneric Event Boundary Detection via Denoising Diffusion‚Äô is accepted to <a href="https://cvpr.thecvf.com" target="_blank" rel="noopener noreferrer">ICCV 2025</a>.
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Feb 27, 2025</th>
                  <td>
                    üìù Our paper ‚ÄòVideo Summarization with Large Language Models‚Äô is accepted to <a href="https://cvpr.thecvf.com" target="_blank" rel="noopener noreferrer">CVPR 2025</a>.

 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Sep 27, 2024</th>
                  <td>
                    üìù Our paper ‚ÄòActFusion: a Unified Diffusion Model for Action Segmentation and Anticipation‚Äô is accepted to <a href="https://nips.cc/" target="_blank" rel="noopener noreferrer">NeurIPS 2024</a>. See you in Vancouver üá®üá¶!
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Aug 2, 2024</th>
                  <td>
                    I serve as an organizer for the <a href="https://sites.google.com/view/wicvaccv2024/" target="_blank" rel="noopener noreferrer">Women in Computer Vision</a> workshop at ACCV 2024, Hanoi, Vietnam.
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Jun 18, 2024</th>
                  <td>
                    <a href="https://arxiv.org/abs/2312.04266" target="_blank" rel="noopener noreferrer">Our paper</a> has been accepted to the <a href="https://sites.google.com/view/lpvl-cvpr2024" target="_blank" rel="noopener noreferrer">LPVL workshop</a> at <a href="https://cvpr.thecvf.com/virtual/2024/index.html" target="_blank" rel="noopener noreferrer">CVPR 2024</a>.
 
                  </td>
                </tr> 
              </table>
            </div> 
          </div>

      
          <!-- Experience -->          <div class="experience">
            <h2>Experience</h2>
            <div class="table-responsive">
              <table class="table table-sm table-borderless"> 
                <tr>
                  <th scope="row">May 2025 - November 2025</th>
                  <td>
                    <a href="http://imagine.enpc.fr" target="_blank" rel="noopener noreferrer">IMAGINE</a>, <a href="https://ecoledesponts.fr" target="_blank" rel="noopener noreferrer">√âcole des Ponts ParisTech</a> (ENPC), France<br>
<em>Visiting Ph.D Student</em>
<ul>
  <li>Host: <a href="https://gulvarol.github.io" target="_blank" rel="noopener noreferrer">G√ºl Varol</a>
</li>
</ul>

 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">March 2020 ‚Äì Present</th>
                  <td>
                    <a href="http://cvlab.postech.ac.kr/lab/" target="_blank" rel="noopener noreferrer">Computer Vision Lab</a>, <a href="https://postech.ac.kr/" target="_blank" rel="noopener noreferrer">POSTECH</a>, Pohang, South Korea<br>
<em>MS/Ph.D student</em>
<ul>
  <li>Advisor: <a href="https://cvlab.postech.ac.kr/~mcho/" target="_blank" rel="noopener noreferrer">Minsu Cho</a>
</li>
</ul>

 
                  </td>
                </tr> 
              </table>
            </div> 
          </div>

              
          <!-- Education -->          <div class="education">
            <h2>Education</h2>
            <div class="table-responsive">
              <table class="table table-sm table-borderless"> 
                <tr>
                  <th scope="row">March 2020 - Present</th>
                  <td>
                    <a href="https://www.postech.ac.kr" target="_blank" rel="noopener noreferrer">Pohang University of Science and Technology (POSTECH)</a>, Pohang, South Korea <br>
M.S. &amp; Ph.D. Student in Graduate School of Artificial Intelligence
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">March 2014 - August 2019</th>
                  <td>
                    <a href="https://www.postech.ac.kr" target="_blank" rel="noopener noreferrer">Pohang University of Science and Technology (POSTECH)</a>, Pohang, South Korea <br>
B.S in Convergence IT Engineering <br>
<!-- Advisor: Prof. [Jaejoon Kim](https://scholar.google.com/citations?user=jdNQRH8AAAAJ&hl=en). -->
 
                  </td>
                </tr> 
              </table>
            </div> 
          </div>

              
          <!-- Selected papers -->
          <div class="publications">
            <h2>Publications</h2>
            <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">            
      <div class="col-sm-4">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/DiffGEBD-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/DiffGEBD-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/DiffGEBD-1400.webp"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/DiffGEBD.png" data-zoomable>

  </picture>

</figure>

      </div>
            
        <!-- Entry bib key -->
        <div id="lee2025llmvs" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Generic Event Boundary Detection via Denoising Diffusion</div>
          <!-- Author -->
          <div class="author">Jaejun Hwang*,¬†
                  <em>Dayoung Gong*</em>,¬†Manjin Kim,¬†and Minsu Cho  
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>IEEE/CVF International Conference on Computer Vision (<b>ICCV</b>),</em> 2025
          </div>
              
            <!---->
           <!-- Additional info -->
            
            <!-- Additional info2 -->
            
      
              
              
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2508.12084" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
            <a href="https://github.com/JaejunHwang/DiffGEBD" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
            <a href="https://jaejunhwang.github.io/DiffGEBD/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Generic event boundary detection (GEBD) aims to identify natural boundaries in a video, segmenting it into distinct and meaningful chunks. Despite the inherent subjectivity of event boundaries, previous methods have focused on deterministic predictions, overlooking the diversity of plausible solutions. In this paper, we introduce a novel diffusion-based boundary detection model, dubbed DiffGEBD, that tackles the problem of GEBD from a generative perspective. The proposed model encodes relevant changes across adjacent frames via temporal self-similarity and then iteratively decodes random noise into plausible event boundaries being conditioned on the encoded features. Classifier-free guidance allows the degree of diversity to be controlled in denoising diffusion. In addition, we introduce a new evaluation metric to assess the quality of predictions considering both diversity and fidelity. Experiments show that our method achieves strong performance on two standard benchmarks, TAPOS and Kinetics-GEBD, generating diverse and plausible event boundaries.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">            
      <div class="col-sm-4">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/LLMVS2-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/LLMVS2-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/LLMVS2-1400.webp"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/LLMVS2.png" data-zoomable>

  </picture>

</figure>

      </div>
            
        <!-- Entry bib key -->
        <div id="lee2025llmvt" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Video Summarization with Large Language Models</div>
          <!-- Author -->
          <div class="author">Min Jung Lee,¬†
                  <em>Dayoung Gong</em>,¬†and Minsu Cho  
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>IEEE/CVF Conference on Computer Vision and Pattern Recogntiion (<b>CVPR</b>),</em> 2025
          </div>
              
            <!---->
           <!-- Additional info -->
            
            <!-- Additional info2 -->
            
      
              
              
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2504.11199" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
            <a href="https://github.com/mlee47/LLMVS" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
            <a href="https://postech-cvlab.github.io/LLMVS/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The exponential increase in video content poses significant challenges in terms of efficient navigation, search, and retrieval, thus requiring advanced video summarization techniques. Existing video summarization methods, which heavily rely on visual features and temporal dynamics, often fail to capture the semantics of video content, resulting in incomplete or incoherent summaries. To tackle the challenge, we propose a new video summarization framework that leverages the capabilities of recent Large Language Models (LLMs), expecting that the knowledge learned from massive data enables LLMs to evaluate video frames in a manner that better aligns with diverse semantics and human judgments, effectively addressing the inherent subjectivity in defining keyframes.
Our method, dubbed  LLM-based Video Summarization (LLMVS), translates video frames into a sequence of captions using an image caption model and then assesses the importance of each frame using an LLM, based on the captions in its local context. These local importance scores are refined through a global attention mechanism in the entire context of video captions, ensuring that our summaries effectively reflect both the details and the overarching narrative. Our experimental results demonstrate the superiority of the proposed method over existing ones in standard benchmarks, highlighting the potential of LLMs in the processing of multimedia content.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">            
      <div class="col-sm-4">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/actfusion_dark-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/actfusion_dark-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/actfusion_dark-1400.webp"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/actfusion_dark.png" data-zoomable>

  </picture>

</figure>

      </div>
            
        <!-- Entry bib key -->
        <div id="gong2024actfusion" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">ActFusion: a Unified Diffusion Model for Action Segmentation and Anticipation</div>
          <!-- Author -->
          <div class="author">
                  <em>Dayoung Gong</em>,¬†Suha Kwak,¬†and Minsu Cho  
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Conference on Neural Information Processing Systems (<b>NeurIPS</b>),</em> 2024
          </div>
              
            <!---->
           <!-- Additional info -->
            
            <!-- Additional info2 -->
            <span style="color:hotpink">BK21 Best Paper Award Winner, 2025</span>
      
              
              
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2412.04353" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
            <a href="https://github.com/gongda0e/ActFusion.git" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Temporal action segmentation and long-term action anticipation are two popular vision tasks for the temporal analysis of actions in videos. 
Despite apparent relevance and potential complementarity, these two problems have been investigated as separate and distinct tasks. In this work, we tackle these two problems, action segmentation and action anticipation, jointly using a unified diffusion model dubbed ActFusion. 
The key idea to unification is to train the model to effectively handle both visible and invisible parts of the sequence in an integrated manner;
the visible part is for temporal segmentation, and the invisible part is for future anticipation. 
To this end, we introduce a new anticipative masking strategy during training in which a late part of the video frames is masked as invisible, and learnable tokens replace these frames to learn to predict the invisible future.
Experimental results demonstrate the bi-directional benefits between action segmentation and anticipation.
ActFusion achieves the state-of-the-art performance across the standard benchmarks of 50 Salads, Breakfast, and GTEA, outperforming task-specific models in both of the two tasks with a single unified model through joint learning.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">            
      <div class="col-sm-4">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/KARI_thumbnail-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/KARI_thumbnail-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/KARI_thumbnail-1400.webp"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/KARI_thumbnail.png" data-zoomable>

  </picture>

</figure>

      </div>
            
        <!-- Entry bib key -->
        <div id="gong2023activity" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Activity Grammars for Temporal Action Segmentation</div>
          <!-- Author -->
          <div class="author">
                  <em>Dayoung Gong*</em>,¬†Joonseok Lee*,¬†Deunsol Jung,¬†Suha Kwak,¬†and Minsu Cho  
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Conference on Neural Information Processing Systems (<b>NeurIPS</b>),</em> 2023
          </div>
              
            <!---->
           <!-- Additional info -->
            
            <!-- Additional info2 -->
            <span style="color:hotpink">LPVL workshop @ CVPR 2024 <br> BK21 Best Paper Award Winner, 2024</span>
      
              
              
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2312.04266" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
            <a href="https://github.com/gongda0e/kari" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
            <a href="https://jsleeo424.github.io/KARI/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Sequence prediction on temporal data requires the ability to understand compositional structures of multi-level semantics beyond individual and contextual properties. The task of temporal action segmentation, which aims at translating an untrimmed activity video into a sequence of action segments,  remains challenging for this reason. 
  This paper addresses the problem by introducing an effective activity grammar to guide neural predictions for temporal action segmentation.  
  We propose a novel grammar induction algorithm that extracts a powerful context-free grammar from action sequence data. We also develop an efficient generalized parser that transforms frame-level probability distributions into a reliable sequence of actions according to the induced grammar with recursive rules. 
  Our approach can be combined with any neural network for temporal action segmentation to enhance the sequence prediction and discover its compositional structure. 
  Experimental results demonstrate that our method significantly improves temporal action segmentation in terms of both performance and interpretability on two standard benchmarks, Breakfast and 50 Salads.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">            
      <div class="col-sm-4">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/futr-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/futr-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/futr-1400.webp"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/futr.png" data-zoomable>

  </picture>

</figure>

      </div>
            
        <!-- Entry bib key -->
        <div id="gong2022future" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Future Transformer for Long-term Action Anticipation</div>
          <!-- Author -->
          <div class="author">
                  <em>Dayoung Gong</em>,¬†Joonseok Lee,¬†Manjin Kim,¬†Seong Jong Ha,¬†and Minsu Cho  
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>IEEE/CVF Conference on Computer Vision and Pattern Recogntiion (<b>CVPR</b>),</em> 2022
          </div>
              
            <!---->
           <!-- Additional info -->
            
            <!-- Additional info2 -->
            <span style="color:hotpink">BK21 Best Paper Award Winner, 2023</span>
      
              
              
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2205.14022" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
            <a href="https://github.com/gongda0e/FUTR" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
            <a href="http://cvlab.postech.ac.kr/research/FUTR/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The task of predicting future actions from a video is crucial for a real-world agent interacting with others.
					When anticipating actions in the distant future,
					we humans typically consider long-term relations over the whole sequence of actions, i.e., not only observed actions in the past but also potential actions in the future.
					In a similar spirit, we propose an end-to-end attention model for action anticipation, dubbed Future Transformer (FUTR), that leverages global attention over all input frames and output tokens to predict a minutes-long sequence of future actions.
					Unlike the previous autoregressive models, the proposed method learns to predict the whole sequence of future actions in parallel decoding, enabling more accurate and fast inference for long-term anticipation.
					We evaluate our method on two standard benchmarks for long-term action anticipation, Breakfast and 50 Salads, achieving state-of-the-art results.</p>
          </div>
        </div>
      </div>
</li>
</ol>
          </div>

          
          <!-- Services -->          <div class="services">
            <h2>Professional Services</h2>
            <div class="table-responsive">
              <table class="table table-sm table-borderless"> 
                <tr>
                  <td>
                    <strong>Technical Chair</strong>
<ul>
  <li>Asian Conference on Computer Vision (<strong>ACCV</strong>), 2024</li>
</ul>

<strong>Workshop Organizer</strong>
<ul>
  <li>Women in Computer Vision in Asian Conference on Computer Vision (<strong>ACCV</strong>), 2024</li>
</ul>

<strong>Conference Reviewer</strong>
<ul>
  <li>Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>), 2024</li>
  <li>European Conference on Computer Vision (<strong>ECCV</strong>), 2024</li>
  <li>International Conference on Computer Vision (<strong>ICCV</strong>), 2023, 2025</li>
  <li>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2022-2024</li>
</ul>

<strong>Journal Reviewer</strong>
<ul>
  <li>IEEE Transactions on Multimedia (<strong>TMM</strong>), 2024-25</li>
  <li>International Journal of Computer Vision (<strong>IJCV</strong>), 2025</li>
</ul>
 
                  </td>
                </tr> 
              </table>
            </div> 
          </div>

          
          <!-- Services -->          <div class="honors">
            <h2>Honors and Awards</h2>
            <div class="table-responsive">
              <table class="table table-sm table-borderless"> 
                <tr>
                  <td>
                    <ul>
  <li>
<strong>BK21 Best Paper Award (Excellence Award)</strong>, POSTECH GSAI, 2023, 2024, 2025</li>
  <li><strong>POSTECH Creative Self-Research Scholarship, 2020 ($5,000)</strong></li>
</ul>
 
                  </td>
                </tr> 
              </table>
            </div> 
          </div>

              

        </article>

</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        ¬© Copyright 2025 Dayoung  Gong. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
  </body>
</html>

